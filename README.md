# ü¶ü Sistema Dengue-Clima: Data Lakehouse Epidemiol√≥gico

> **Status:** Em Desenvolvimento (Fase de Implementa√ß√£o na AWS) üöß
> **Stack:** Python, Airflow (Astronomer), Spark, AWS (S3, Glue, Athena), Docker.

## 1. Vis√£o do Projeto (Business Case)
O **Sistema Dengue-Clima** √© uma plataforma de Engenharia de Dados projetada para correlacionar dados epidemiol√≥gicos (Dengue, Zika) com dados clim√°ticos (Chuva, Temperatura). O objetivo √© fornecer uma base de dados anal√≠tica (Gold Layer) para prever surtos de arboviroses baseados em padr√µes meteorol√≥gicos, utilizando uma arquitetura de Lakehouse na AWS.

**Fontes de Dados:**
1.  **InfoDengue API** (Dados epidemiol√≥gicos semanais).
    *   Hist√≥rico: √öltimos 10 anos.
    *   Granularidade: Munic√≠pio/Semana.
2.  **INMET API** (Dados meteorol√≥gicos hor√°rios/di√°rios).
    *   Vari√°veis: Temperatura, Precipita√ß√£o, Umidade.
    *   Hist√≥rico: Alinhado com dados epidemiol√≥gicos.
3.  **IBGE API** (Dados Demogr√°ficos e Geogr√°ficos).
    *   Popula√ß√£o Anual (para c√°lculo de incid√™ncia).
    *   Malha Geogr√°fica (Lat/Long).

---

## 2. Escopo e Objetivos
O escopo do projeto √© construir um pipeline de dados ponta a ponta, desde a ingest√£o de dados brutos at√© a camada de agrega√ß√£o, pronta para consumo por ferramentas de Business Intelligence e Machine Learning.

**Objetivos:**
- **Ingest√£o Automatizada:** Coletar dados de forma programada e confi√°vel.
- **Arquitetura Lakehouse:** Implementar as camadas Bronze, Silver e Gold em um Data Lake na AWS.
- **Qualidade de Dados:** Garantir que os dados sejam limpos, consistentes e prontos para an√°lise.
- **Escalabilidade:** Construir uma solu√ß√£o que suporte o crescimento do volume de dados.
- **An√°lise de Dados:** Permitir a correla√ß√£o entre dados de dengue e clima para gerar insights.

---

## 3. Funcionalidades Implementadas
- **Orquestra√ß√£o de DAGs com Airflow:** Pipelines de ingest√£o e processamento de dados.
- **Arquitetura Medallion Local:** Estrutura de dados em camadas (Bronze, Silver, Gold) simulada localmente.
- **Ingest√£o de Dados:** Conectores para as APIs do InfoDengue e INMET.
- **Processamento de Dados:** Scripts para limpeza, transforma√ß√£o e enriquecimento dos dados.
- **Containeriza√ß√£o:** Ambiente de desenvolvimento local com Docker e Astro CLI.

---

## 4. Pr√≥ximas Etapas e Tarefas Pendentes
- **Migra√ß√£o para AWS:**
    - Configurar o armazenamento de dados no Amazon S3 para as camadas do Lakehouse.
    - Adaptar os pipelines de dados para usar AWS Glue para ETL.
    - Utilizar o Amazon Athena para consultas ad-hoc na camada Gold.
- **Melhorias nos Conectores:**
    - Implementar l√≥gica de retentativas (retry) e tratamento de erros nos conectores de API.
- **Monitoramento e Alertas:**
    - Configurar alertas para falhas nos pipelines de dados.
- **Documenta√ß√£o:**
    - Detalhar o dicion√°rio de dados da camada Gold.

---

## 5. Requisitos do Sistema e Depend√™ncias
- **Desenvolvimento Local:**
    - Docker Desktop
    - Astro CLI
    - Python 3.9+
- **Produ√ß√£o (AWS):**
    - Conta na AWS
    - Servi√ßos: S3, Glue, Athena

---

## 6. Instru√ß√µes de Configura√ß√£o e Execu√ß√£o

### Ambiente de Desenvolvimento Local

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/Allanmagnoo/sistema-dengue-clima.git
    cd sistema-dengue-clima
    ```

2.  **Inicie o Ambiente Local com Airflow:**
    ```bash
    astro dev start
    ```
    Acesse a interface do Airflow em: `http://localhost:8080` (usu√°rio: `admin`, senha: `admin`).

3.  **Instale as depend√™ncias locais para desenvolvimento:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # ou .venv\Scripts\activate no Windows
    pip install -r requirements.txt
    ```

---

## 7. Equipe e Contatos
- **Desenvolvedor Principal:** Allan Magno
- **Contato:** allanmagno@gmail.com
- **GitHub:** [https://github.com/Allanmagnoo](https://github.com/Allanmagnoo)
