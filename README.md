# ðŸ¦Ÿ Eco-Sentinel: Data Lakehouse EpidemiolÃ³gico

> **Status:** Em Desenvolvimento ðŸš§
> **Stack:** Python, Airflow (Astronomer), Spark, AWS (Simulado Localmente), Docker.

## 1. VisÃ£o do Projeto (Business Case)
O **Eco-Sentinel** Ã© uma plataforma de Engenharia de Dados projetada para correlacionar dados epidemiolÃ³gicos (Dengue, Zika) com dados climÃ¡ticos (Chuva, Temperatura) em tempo real.
O objetivo Ã© fornecer uma base de dados analÃ­tica (Gold Layer) para prever surtos de arboviroses baseados em padrÃµes meteorolÃ³gicos.

**Fontes de Dados:**
1.  **InfoDengue API** (Dados epidemiolÃ³gicos semanais).
2.  **INMET API** (Dados meteorolÃ³gicos horÃ¡rios).

---

## 2. Arquitetura TÃ©cnica (Medallion)

O projeto segue a arquitetura MedalhÃ£o (Lakehouse):

| Camada | Formato | DescriÃ§Ã£o |
| :--- | :--- | :--- |
| **Bronze** | JSON (Raw) | Dados brutos extraÃ­dos das APIs. ImutÃ¡veis. Particionados por `source/year`. |
| **Silver** | Parquet | Dados limpos, tipados, deduplicados e enriquecidos. Schema enforcement aplicado. |
| **Gold** | Parquet | Dados agregados (KPIs). Tabela Ãºnica (One Big Table) pronta para Dashboards. |

**Infraestrutura:**
* **OrquestraÃ§Ã£o:** Apache Airflow 2.x (via Astro CLI).
* **ContainerizaÃ§Ã£o:** Docker.
* **Linguagem:** Python 3.9+.

---

## 3. Guia de ConfiguraÃ§Ã£o (Quick Start)

### PrÃ©-requisitos
* Docker Desktop (Running)
* Astro CLI instalado
* Python 3.9+

### InstalaÃ§Ã£o

1.  **Clone o repositÃ³rio:**
    ```bash
    git clone [https://github.com/seu-usuario/eco-sentinel.git](https://github.com/seu-usuario/eco-sentinel.git)
    cd eco-sentinel
    ```

2.  **Inicie o Ambiente Local:**
    ```bash
    astro dev start
    ```
    *Acesse o Airflow UI em: `http://localhost:8080` (User: admin / Pass: admin)*

3.  **Instale dependÃªncias locais (para desenvolvimento no VS Code):**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # ou .venv\Scripts\activate no Windows
    pip install -r requirements.txt
    ```

---

## 4. Roadmap de ExecuÃ§Ã£o (Step-by-Step)

### FASE 1: IngestÃ£o (Bronze Layer) ðŸ› ï¸
- [ ] **Configurar Conectores**: Implementar scripts em `src/connectors/` com retry logic.
    - `infodengue_api.py`: Busca dados por geocode/ano.
    - `inmet_api.py`: Busca dados de estaÃ§Ãµes automÃ¡ticas.
- [ ] **Criar DAGs de IngestÃ£o**:
    - `dags/ingest_dengue_historical.py`: Backfill de 5 anos.
    - `dags/ingest_daily_weather.py`: ExecuÃ§Ã£o diÃ¡ria (D-1).
- [ ] **Validar Bronze**: Verificar se os JSONs estÃ£o sendo salvos em `data/bronze/`.

### FASE 2: Refinamento (Silver Layer) ðŸ§¹
- [ ] **Processamento Spark/Pandas**:
    - Ler JSONs da Bronze.
    - Tratamento de Tipagem (String -> Date/Float).
    - Limpeza de Outliers (Ex: Temperaturas > 60Â°C).
- [ ] **Escrita Parquet**: Salvar em `data/silver` particionado por `UF`.

### FASE 3: AgregaÃ§Ã£o (Gold Layer) ðŸ“Š
- [ ] **Regras de NegÃ³cio**:
    - Agregar Clima (HorÃ¡rio) -> Semanal (MÃ©dia/MÃ¡x/MÃ­n).
    - Join `Dengue` + `Clima` via chaves `Geocode` e `Semana EpidemiolÃ³gica`.
- [ ] **CriaÃ§Ã£o de Features**:
    - Calcular *Lags* (Chuva de 2 semanas atrÃ¡s).

### FASE 4: VisualizaÃ§Ã£o ðŸ“ˆ
- [ ] Conectar ferramenta de Data Viz (Streamlit ou Metabase) ao Data Lake.
- [ ] Criar GrÃ¡fico de CorrelaÃ§Ã£o (Curva de Chuva x Curva de Casos).

---

## 5. Estrutura de DiretÃ³rios

```text
eco-sentinel/
â”œâ”€â”€ dags/                  # Pipelines do Airflow
â”œâ”€â”€ data/                  # Data Lake Local (Gitignored)
â”œâ”€â”€ include/               # Arquivos de config auxiliares
â”œâ”€â”€ src/                   # LÃ³gica de NegÃ³cio (ETL Core)
â”‚   â”œâ”€â”€ connectors/        # Scripts de extraÃ§Ã£o
â”‚   â””â”€â”€ common/            # Logs e UtilitÃ¡rios
â”œâ”€â”€ tests/                 # Testes UnitÃ¡rios
â”œâ”€â”€ Dockerfile             # ConfiguraÃ§Ã£o da imagem Astro
â””â”€â”€ requirements.txt       # Libs Python do Airflow