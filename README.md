# ğŸ¦Ÿ Sistema Dengue-Clima: Data Lakehouse EpidemiolÃ³gico

> **Status:** Em MigraÃ§Ã£o para Google Cloud Platform (GCP) ï¿½
> **Stack:** Python, Dataform (SQLX), BigQuery, Cloud Composer (Airflow), Vertex AI, Looker Studio.

## 1. VisÃ£o do Projeto (Business Case)

O **Sistema Dengue-Clima** Ã© uma plataforma de Engenharia de Dados projetada para correlacionar dados epidemiolÃ³gicos (Dengue, Zika) com dados climÃ¡ticos (Chuva, Temperatura). O objetivo Ã© fornecer uma base de dados analÃ­tica (Gold Layer) para prever surtos de arboviroses baseados em padrÃµes meteorolÃ³gicos, utilizando uma arquitetura moderna na nuvem.

**Fontes de Dados:**

1. **InfoDengue API** (Dados epidemiolÃ³gicos semanais).
    * HistÃ³rico: Ãšltimos 10 anos.
    * Granularidade: MunicÃ­pio/Semana.
2. **INMET API** (Dados meteorolÃ³gicos horÃ¡rios/diÃ¡rios).
    * VariÃ¡veis: Temperatura, PrecipitaÃ§Ã£o, Umidade.
    * HistÃ³rico: Alinhado com dados epidemiolÃ³gicos.
3. **IBGE API** (Dados DemogrÃ¡ficos e GeogrÃ¡ficos).
    * PopulaÃ§Ã£o Anual (para cÃ¡lculo de incidÃªncia).
    * Malha GeogrÃ¡fica (Lat/Long).

---

## 2. Escopo e Objetivos

O escopo do projeto Ã© construir um pipeline de dados ponta a ponta, desde a ingestÃ£o de dados brutos atÃ© a camada de agregaÃ§Ã£o, pronta para consumo por ferramentas de Business Intelligence e Machine Learning.

**Objetivos:**
* **IngestÃ£o Automatizada:** Coletar dados de forma programada e confiÃ¡vel.
* **Arquitetura Lakehouse:** Implementar as camadas Bronze, Silver e Gold no BigQuery.
* **Qualidade de Dados:** Garantir que os dados sejam limpos, consistentes e prontos para anÃ¡lise usando Dataform Assertions.
* **Escalabilidade:** Utilizar serviÃ§os serverless do GCP para suportar grandes volumes de dados.
* **AnÃ¡lise de Dados:** Permitir a correlaÃ§Ã£o entre dados de dengue e clima para gerar insights via Looker Studio.

---

## 3. Arquitetura (GCP)

A arquitetura do projeto foi migrada da AWS para o Google Cloud Platform para aproveitar recursos nativos de Big Data e ML.

### ğŸ“Š **Bronze Layer** (Raw Data)
* **Armazenamento:** Google Cloud Storage (GCS) / BigQuery (External Tables)
* **Formato:** JSON/CSV originais
* **Processo:** IngestÃ£o via Cloud Functions ou Cloud Composer (Airflow)

### ğŸ”„ **Silver Layer** (Refined Data)
* **Armazenamento:** BigQuery (Native Tables)
* **Ferramenta de TransformaÃ§Ã£o:** Dataform (SQLX)
* **Processos:**
    * Limpeza de dados
    * PadronizaÃ§Ã£o de tipos
    * DeduplicaÃ§Ã£o
    * Enriquecimento com dados geogrÃ¡ficos

### ğŸ† **Gold Layer** (Analytics Ready)
* **Armazenamento:** BigQuery
* **Ferramenta de TransformaÃ§Ã£o:** Dataform (SQLX)
* **Modelos:**
    * Marts dimensionais (Star Schema)
    * Tabelas agregadas para dashboards
* **ML Integration:** Vertex AI / BigQuery ML para previsÃµes de surtos

### ğŸ“ˆ **VisualizaÃ§Ã£o**
* **Ferramenta:** Looker Studio
* **ConexÃ£o:** Direta com BigQuery

---

## 4. Justificativa da MigraÃ§Ã£o (AWS â†’ GCP)

A infraestrutura foi migrada da AWS para o GCP visando otimizaÃ§Ã£o de custos e integraÃ§Ã£o facilitada de ferramentas de dados.

* **IntegraÃ§Ã£o Nativa:** O uso do **Dataform** integrado ao BigQuery simplifica drasticamente a gestÃ£o de dependÃªncias e transformaÃ§Ãµes SQL, substituindo scripts complexos em Python/Glue.
* **Serverless First:** O BigQuery oferece uma capacidade de processamento serverless que elimina a necessidade de gerenciamento de clusters (como no EMR/Glue), reduzindo o overhead operacional.
* **Machine Learning:** A integraÃ§Ã£o direta do BigQuery com o **Vertex AI** facilita a criaÃ§Ã£o e deploy de modelos preditivos sem movimentaÃ§Ã£o excessiva de dados.
* **HistÃ³rico:** A versÃ£o anterior do projeto utilizava AWS S3, Glue e Athena. Essa experiÃªncia serviu de base para a modelagem atual, mas a stack GCP provou-se mais Ã¡gil para este caso de uso especÃ­fico.

---

## 5. Requisitos de ConfiguraÃ§Ã£o (GCP)

Para executar este projeto no ambiente GCP, sÃ£o necessÃ¡rios:

1. **Conta Google Cloud:**
   * Projeto ativo com billing habilitado.
   * APIs habilitadas: BigQuery API, Dataform API, Cloud Storage API, Vertex AI API.

2. **Ferramentas Locais:**
   * [Google Cloud SDK (gcloud)](https://cloud.google.com/sdk/docs/install)
   * [Dataform CLI](https://cloud.google.com/dataform/docs/use-dataform-cli) (opcional, para dev local)
   * Python 3.9+

3. **PermissÃµes (IAM):**
   * O usuÃ¡rio ou Service Account deve ter permissÃµes de `BigQuery Data Editor`, `BigQuery Job User` e `Dataform Editor`.

---

## 6. Guia de ImplantaÃ§Ã£o

### ConfiguraÃ§Ã£o Inicial

1. **AutenticaÃ§Ã£o:**
   ```bash
   gcloud auth application-default login
   gcloud config set project SEU_PROJETO_GCP
   ```

2. **Setup do Dataform:**
   * Navegue atÃ© a pasta `etl_project`.
   * Configure o arquivo `dataform.json` com o ID do seu projeto GCP.
   * Instale as dependÃªncias:
     ```bash
     npm install
     ```

3. **ExecuÃ§Ã£o do Pipeline (Manual):**
   ```bash
   dataform run
   ```

### Deploy AutomÃ¡tico

O deploy contÃ­nuo Ã© gerenciado via repositÃ³rio conectado ao Dataform no console do GCP.

1. Conecte o repositÃ³rio Git ao Dataform no Console GCP.
2. Crie um "Release Configuration" apontando para a branch `main`.
3. Crie um "Workflow Configuration" para agendar as execuÃ§Ãµes (ex: DiÃ¡rio Ã s 06:00 UTC).

---

## 7. Estrutura do Projeto (Atualizada)

```
sistema-dengue-clima/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataform/               # Projeto Dataform (Novo Core ETL)
â”‚   â”‚   â”œâ”€â”€ definitions/        # DeclaraÃ§Ãµes SQLX
â”‚   â”‚   â”‚   â”œâ”€â”€ bronze/         # DeclaraÃ§Ãµes de fontes
â”‚   â”‚   â”‚   â”œâ”€â”€ silver/         # TransformaÃ§Ãµes intermediÃ¡rias
â”‚   â”‚   â”‚   â”œâ”€â”€ gold/           # Modelos finais
â”‚   â”‚   â”‚   â””â”€â”€ assertions/     # Testes de qualidade de dados
â”‚   â”‚   â”œâ”€â”€ dataform.json       # ConfiguraÃ§Ã£o do Dataform
â”‚   â”‚   â””â”€â”€ package.json        # DependÃªncias JS
â”‚   â”œâ”€â”€ jobs/                   # Scripts Python Legados (DuckDB/Pandas)
â”‚   â””â”€â”€ app/                    # AplicaÃ§Ã£o Streamlit
â”œâ”€â”€ dags/                       # DAGs do Airflow (Cloud Composer)
â”œâ”€â”€ docs/                       # DocumentaÃ§Ã£o
â””â”€â”€ README.md                   # Este arquivo
```

---

## 8. Equipe e Contatos

* **Desenvolvedor Principal:** Allan Magno
* **Contato:** <allanmagno@gmail.com>
* **GitHub:** [https://github.com/Allanmagnoo](https://github.com/Allanmagnoo)
* **Suporte GCP:** Para questÃµes relacionadas Ã  infraestrutura GCP, abra uma issue neste repositÃ³rio com a tag `gcp-infra`.
